{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dataiku.com/static/img/learn/guide/getting-started/getting-started-with-python/logo-stack-python.png\" style=\"width: 700px;\">\n",
    "<h1 align=center style=\"color: #005496; font-size: 4.2em;\">Machine Learning</h1>\n",
    "<h2 align=center>Laboratory on Numpy / Matplotlib / Pandas / Scikit-learn</h2>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Python has become the de-facto standard programming language for data analytics in the past few years. Python's success is due to several factors, but one primary reason has been the availability of robust, open-source libraries for scientific computation, such as Numpy, Scipy and Matplotlib. Python is also the most popular programming language for machine learning, thanks to libraries such as Scikit-learn, TensorFlow and PyTorch.\n",
    "\n",
    "This lecture will explore the basics of Numpy, Matplotlib and Scikit-learn. The first is a library for data manipulation through the powerful `numpy.ndarray` data structure; the second is helpful for graphical visualization and plotting; the third is a general-purpose library for machine learning, containing dozens of algorithms for classification, regression, clustering and others.\n",
    "\n",
    "In this lecture, we assume familiarity with the Python programming language. If you are unfamiliar with the language, we advise you to look it up before going over to the next sections. Here are some useful links to learn about Python:\n",
    "- https://docs.python.org/3/tutorial/introduction.html\n",
    "- https://www.learnpython.org/\n",
    "- http://www.scipy-lectures.org/\n",
    "\n",
    "If you have never seen a page like this, it is a **Jupyter Notebook**. Here one can easily embed Python code and run it on the fly. You can run the code in a cell by selecting the cell and clicking the *Run* button (top). You can do the same using the **SHIFT+Enter** shortcut. You can modify the existing cells, run them, and save your changes.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. Python (preferably version > 3.7): https://www.python.org/downloads/\n",
    "2. Numpy, Scipy and Matplotlib: https://www.scipy.org/install.html\n",
    "3. Scikit-learn: http://scikit-learn.org/stable/install.html\n",
    "4. Pandas: https://pandas.pydata.org/docs/getting_started/index.html\n",
    "\n",
    "## References\n",
    "\n",
    "- https://docs.scipy.org/doc/numpy/\n",
    "- https://docs.scipy.org/doc/scipy/reference/\n",
    "- https://matplotlib.org/users/index.html\n",
    "- http://scikit-learn.org/stable/documentation.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy\n",
    "\n",
    "Numpy provides high-performance data structures for data manipulation and numeric computation. In particular, we will look at the `numpy.ndarray`, a data structure for manipulating vectors, matrices and tensors. Let's start by importing `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize a Numpy array from a Python list using the `numpy.array` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an integer array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a 2-dimensional matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a Numpy array, we can check its `shape`, a tuple containing the number of elements for each dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the shape of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the shape of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do quite some nice things with Numpy arrays that are not possible with standard Python lists.\n",
    "\n",
    "### Indexing\n",
    "Numpy array allow us to index arrays in quite advanced ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the first element of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract from the first to the third element of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract values from the array using a boolean mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of Numpy indexing capabilities starts showing up with 2d arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access a single element of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access an entire row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access an entire column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract a sub-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation\n",
    "We can manipulate data in several ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Flatten a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reshape a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the max and the min of the matrix/vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation of the matrix/vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a couple of functions which will be useful to plot the decision function of a trained ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.lib import plot_data\n",
    "from utils.lib import plot_decision_surface\n",
    "from utils.lib import plot_3D_decision_surface\n",
    "from utils.lib import plot_svm_margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings within the notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "# Scikit-learn\n",
    "Let's now dive into the real **Machine Learning** part. *Scikit-learn* is the most widespread library for Machine Learning in use nowadays, and most of its fame is due to its extreme simplicity. With Scikit-learn, it is possible to manage datasets easily and train a wide range of classifiers out-of-the-box. It is also helpful for several other Machine Learning tasks, such as regression, clustering, dimensionality reduction, and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Summary\n",
    "\n",
    "1. Classification\n",
    "    1. SVM (Linear, Feature Mapping, Kernel)\n",
    "    2. Decision Tree\n",
    "    3. Real Case: Glass Identification\n",
    "2. K-Fold Cross Validation\n",
    "3. Clustering\n",
    "    1. K-means\n",
    "4. Dimensionality Reduction\n",
    "    1. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a suitable dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first generate a synthetic dataset which we will use for the experiments. We will restrict ourselves to a simple case, in which each example has 2-components.\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\{ x_0, x_1\\} \\qquad x_0, x_1 \\in \\mathcal{N}(0,1) \\qquad \\mathbf{X} = \\{\\mathbf{x}_i\\}^{N}_{i=0} \\\\[1.5ex]\n",
    "$$\n",
    "Then, we assume that each example $\\mathbf{x}$ is part of the dataset if and only if it satisfies the following condition (remember that $x^2 +y^2 = r^2$ is the equation describing a circle centered at the origin).\n",
    "$$\n",
    "\\mathbf{x} \\in \\mathbf{X} \\quad \\Leftrightarrow \\quad (x_0^2 + x_1^2) > 1 \\;\\; \\vee \\;\\; (x_0^2 + x_1^2) \\leq 0.25 \\\\[1.5ex]\n",
    "$$\n",
    "\n",
    "Lastly, we need to classify these points. We assume that the classification function has the following form:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{cases} \n",
    "1 \\quad x_0^2 + x_1^2 > 1\\\\\n",
    "0 \\quad x_0^2 + x_1^2 \\leq 0.25\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a dataset and plot the correspoding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset by using the utils.lib.generate_data function\n",
    "# Plot the generated data by ysing the utils.lib.plot_data function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the data splitting between train and test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM and then perform inference over the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We trained our first model. Let us evaluate the model over the test set. As a simple evaluation metric, we will use the accuracy. Remember that the accuracy metric is defined as:\n",
    "\n",
    "$$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy over the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the performances are not that impressive. For such a simple dataset we would expect to hit more than 90% accuracy. Something is clearly wrong. Let us have a look at the shape of the learned decision function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision surface of the SVM using the utils.lib.plot_decision_surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the margin of the SVM using the utils.lib.plot_svm_margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the decision function is linear, while the data presents a non-linearity. Therefore, we need to employ a different kind of kernel to capture this feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature mapping\n",
    "Non-linearly separable problems need a higher expressive power. We employ a homogeneus feature mapping $\\phi: \\mathcal{X} \\rightarrow \\mathcal{H}$ which maps each example $\\mathbf{x} \\in \\mathcal{X}$ in a higher-dimensional space $\\mathcal{H}$. The examples must be (approximately) linearly separable in the mapped space.\n",
    "\n",
    "$$\n",
    "    \\phi : \\mathbf{R}^2 \\rightarrow \\mathbf{R}^3\\\\\n",
    "    \\mathbf{x} = \\binom{x_0}{x_1} \\qquad \\phi(\\mathbf{x}) = \\left(\\begin{gather}\n",
    "    x_0^2 \\\\\n",
    "    x_0 \\cdot x_1 \\\\\n",
    "    x_1^2\n",
    "  \\end{gather}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which converts a 2D example in its 3D mapping\n",
    "# Create a new variable called X3d which contains the mapped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train/test split and save the new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with Linear Kernel (Feature Mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM and perform inference with the mapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the 3D decision surface of the SVM using utils.lib.plot_3D_decision_surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Polynomial Kernel\n",
    "\n",
    "Devising a feature mapping $\\phi$ is a nice idea. However, it can be time-consuming and expensive to compute if we are dealing with a high-dimensional polynomial mapping. We can use the **kernel trick** to avoid computing explicity the mapping.\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x'}) = \\phi(\\mathbf{x}) \\phi(\\mathbf{x}') \\qquad \\phi: \\mathcal{X} \\rightarrow \\mathcal{H} \\quad \\mathbf{x} \\in \\mathbb{X}\n",
    "$$\n",
    "\n",
    "If we use a kernel, then we work directly on the _input space_ rathen than a different mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an SVM by using a (homogeneous) polynomial kernel of degree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision surface of the SVM using utils.lib.plot_decision_surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train/test split and save the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier and train it over the data\n",
    "# Perform inference over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are models which we can consider interpretable. It means that we can \"look into\" the model itself and understand how the decision function. In the case of decision trees, sklearn provides us with a nice utility to plot a trained tree to understanding the splitting rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an interpretable version of the trained decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Case: Glass Identification\n",
    "\n",
    "Let us examine a real-world dataset called \"Glass Identification Dataset\" [1]. It consists of 10 real-valued attributes (chemical components) and we are asked to predict the type of class of the examples (e.g., tableware, headlamp, etc.). Such analysis is extremely useful. For example, when examining evidence for a criminal trial.\n",
    "\n",
    "In this exercise, we will transform this into a binary classification problem (one-vs-all). We want to identify if a piece of glass comes from \"tableware\" or not.\n",
    "\n",
    "[1] https://archive.ics.uci.edu/ml/datasets/glass+identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Machine Learning Workflow\n",
    "\n",
    "1. Analyze your dataset and the task (e.g., understand data distribution, check for missing values, etc.)\n",
    "1. Preprocess your dataset (e.g., input missing values, standardize, etc.)\n",
    "1. Choose a suitable model (e.g., decision tree, SVM, neural network, etc.)\n",
    "1. Train your model (hyperparameter tuning)\n",
    "1. Evaluate your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset data/glass.csv using pandas\n",
    "# Print some information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable (Y) from the features X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfom the train test split over this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We standardize our data (remove the mean, divide by the std). This is done by only looking at the train set.\n",
    "# We can also standardize the test set by using the mean/std coming from the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM and perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The accuracy of our model is very high. It is more or less what we want. However, how do we know that our model is doing well? Visual inspection is not possible anymore, since we have a higher dimensional problem. Therefore, we need to resort to additional metrics with a higher \"explanatory\" power.\n",
    "\n",
    "$F_1$ is defined as the harmonic mean of the precision and recall. It is a metric which measures the test's accuracy.\n",
    "\n",
    "$$\n",
    "    \\begin{gather}\n",
    "    F_1 = 2 \\cdot \\dfrac{(precision \\cdot recall)}{(precision + recall)} \\\\[1.5em]\n",
    "    precision = \\frac{TP}{TP+FP} \\qquad recall = \\frac{TP}{TP+FN}\n",
    "    \\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $F_1$ score is really bad. Let us check more in detail what is happening with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the precision and recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report is kind of useful, but the **confusion matrix** can give us a better picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is able to predict correctly all the instances of class $0$. However, it is very bad in classifying instances of the class $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the class predicted by the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the target class distribution in the train dataset (y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the standard formulation of a soft-margin SVM. The regularization parameter $C$ trade-offs the data fitting with the size of the margin. In the standard case, $C$ is the same for all the classes, which means that the misclassification penalty is the same for every class.\n",
    "$$\n",
    "\\min_{\\mathbb{w}, w_0, \\zeta} = ||\\mathbf{w}||^2 + C \\sum_{i=0}^N \\zeta_i\n",
    "$$\n",
    "\n",
    "In our case, since we are dealing with an unbalanced dataset, we want to give a higher penalty if we misclassify an example coming from the minority class. Therefore, the formulation becomes:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbb{w}, w_0, \\zeta} = ||\\mathbf{w}||^2 + \\sum_{i=0}^N \\zeta_i \\cdot (\\mathbb{I}[y_i = 1]C_{1}+\\mathbb{I}[y_i = 0]C_{0})\n",
    "$$\n",
    "\n",
    "where $C_{0}$ and $C_{1}$ indicates the penalties associated to the two classes. $\\mathbb{I}$ is the indicator function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a balanced SVM and perform inference over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the precision and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real word, we do not have a separate test set we can evaluate on. We need a way to evaluate our model and obtain a reasonable estimate about its predictive quality once sent to production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a dataset such as this one:\n",
    "<br><br>\n",
    "<img src=\"./img/kfold/1.png\" style=\"width: 400px;\"/>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<img src=\"./img/kfold/2.png\" style=\"width: 400px;\"/>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we use the train set to train our model. Then, we evaluate on the test set. However, in real life, we do not have a test set which is representative of the real distribution the model will have to work on once deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<img src=\"img/kfold/3.png\" style=\"width: 400px;\"/>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do? We could split again the train set and obtain a **validation set** we could use for the evaluation. Basically, we forget that the test dataset exists and we use it only at the end.  \n",
    "\n",
    "<br><br>\n",
    "<img src=\"img/kfold/4.png\" style=\"width: 400px;\"/>\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "<img src=\"img/kfold/5.png\" style=\"width: 400px;\"/>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<img src=\"img/kfold/6.png\" style=\"width: 400px;\"/>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, why should we limit ourselves to a single validation dataset? We can take \"multiple\" validation datasets and compute an average of the performance of our model. \n",
    "\n",
    "<br><br>\n",
    "<img src=\"img/kfold/7.png\" style=\"width: 400px;\"/>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<img src=\"img/kfold/8.png\" style=\"width: 400px;\"/>\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "<img src=\"img/kfold/9.png\" style=\"width: 400px;\"/>\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "<img src=\"img/kfold/10.png\" style=\"width: 400px;\"/>\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on an SVM classifier.\n",
    "# Use as scoring metric the F1\n",
    "# Save the F1 values for each split into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the k-fold scores and their mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an SVM model using the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the final F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume to have a $d$-dimensional dataset $\\mathbf{X}$. We want to partition this dataset in $K$ different partitions (or clusters) in an unsupervised manner. More formally:\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{R}^d \\qquad \\mathbf{X} = \\{x_i\\}_{i=0}^{N} \\qquad K \\in \\mathbb{N}^{+}\\\\[1.5em]\n",
    "\\mathbf{X} = \\bigcup_{i=0}^{K} S_i \\qquad S_i \\subseteq X \\;  \\wedge \\; S_i \\cap S_j = \\varnothing \\quad \\forall \\; i \\neq j\n",
    "$$\n",
    "\n",
    "Each partition $S_i \\subset X$ has a centroid $c_i$. A centroid represents the arithmetic mean of all the points of  the given partition. Therefore, as for the optimization problem, we want to minimize the following quantity:\n",
    "  \n",
    "$$ \n",
    "    \\underset{K \\rightarrow \\{S_0, \\ldots, S_K\\}}{\\mathrm{argmin}} \\sum_{i=1}^K \\sum_{x \\in S_i} d(x, c_i) \\qquad d(x,c_i) = || x - c_i||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/kmean_steps/1.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Start with random clusters\n",
    "\n",
    "<img src=\"img/kmean_steps/2.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Assign points to the closest centroid\n",
    "<img src=\"img/kmean_steps/3.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Update centroid values\n",
    "<img src=\"img/kmean_steps/4.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Assign points to the closest centroid\n",
    "<img src=\"img/kmean_steps/5.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Update centroid values\n",
    "<img src=\"img/kmean_steps/6.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Case: UrbanGB\n",
    "\n",
    "This dataset contains the coordinates (longitude and latitude) of 360177 road accidents occurred in urban areas in Great Britain [1]. Obvioulsy, road accidents are concentrare around urban centers. The idea is to try to cluster these data to find these \"urban centers\". \n",
    "\n",
    "\n",
    "[1] https://archive.ics.uci.edu/ml/datasets/UrbanGB%2C+urban+road+accidents+coordinates+labelled+by+the+urban+center\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Utility function to assign each point to the predicted cluster\n",
    "def create_cluster_dict(pred,X):\n",
    "    tmp = { }\n",
    "    for c, x in zip(pred, X.to_numpy()):\n",
    "        tmp.update({c: tmp.get(c, [])+[x]})\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import with pandas the data/urbanGB.csv dataset\n",
    "# Sample from it the 2% of the data\n",
    "# Get some information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a KMeans classifier and perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters\n",
    "clusters = create_cluster_dict(pred,X)\n",
    "\n",
    "for i in clusters.values():\n",
    "    i = np.array(i)\n",
    "    plt.scatter(i[:,0],i[:,1])\n",
    "\n",
    "plt.scatter(clustering.cluster_centers_[:,0],clustering.cluster_centers_[:,1],s=100,color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the clustering result when varying the number of clusters\n",
    "plt.figure(figsize=(18,4))\n",
    "\n",
    "for i in range(2,6):\n",
    "    \n",
    "    clustering = KMeans(n_clusters=i)\n",
    "    pred = clustering.fit_predict(X)\n",
    "    \n",
    "    plt.subplot(1,5,i-1)\n",
    "    plt.ylabel(\"Longitude\")\n",
    "    plt.xlabel(\"Latitude\")\n",
    "    \n",
    "    clusters = create_cluster_dict(pred,X)\n",
    "    for i in clusters.values():\n",
    "        i = np.array(i)\n",
    "        plt.scatter(i[:,0],i[:,1])\n",
    "    plt.scatter(clustering.cluster_centers_[:,0],clustering.cluster_centers_[:,1],s=100,color=\"red\", marker=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many clusters? Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the inertia varying the number of clusters from 1 to 10\n",
    "# Save the inertia inside a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a KMeans classifier using the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Dataset: Wine Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines [1].\n",
    "\n",
    "[1] https://archive.ics.uci.edu/ml/datasets/wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into target and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a PCA object and transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variace ratio for each component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with the K-means example, the point of inflexion (where the line starts to bend) should indicate how many components have to be retained. In this case, the magic number is 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (2-dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a PCA object (2 components) and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data by using the utils.lib.plot_pca_clusters function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (3-dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a PCA object (3 components) and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data by using the utils.lib.plot_pca_clusters function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
